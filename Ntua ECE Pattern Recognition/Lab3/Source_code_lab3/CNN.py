# -*- coding: utf-8 -*-
"""convolutionalnnteliko.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x1kHFEozZxfFULJibGjkUx2TEFjIkDcc
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
     #   print(os.path.join(dirname, filename))
     I=0
# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import copy
import os

import numpy as np
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler

# HINT: Use this class mapping to merge similar classes and ignore classes that do not work very well
Class_Mapping = {
    "Rock": "Rock",
    "Psych-Rock": "Rock",
    "Indie-Rock": None,
    "Post-Rock": "Rock",
    "Psych-Folk": "Folk",
    "Folk": "Folk",
    "Metal": "Metal",
    "Punk": "Metal",
    "Post-Punk": None,
    "Trip-Hop": "Trip-Hop",
    "Pop": "Pop",
    "Electronic": "Electronic",
    "Hip-Hop": "Hip-Hop",
    "Classical": "Classical",
    "Blues": "Blues",
    "Chiptune": "Electronic",
    "Jazz": "Jazz",
    "Soundtrack": None,
    "International": None,
    "Old-Time": None,
}


def torch_train_val_split(
    dataset, batch_train, batch_eval, val_size=0.3, shuffle=True, seed=420
):
    # Creating data indices for training and validation splits:
    dataset_size = len(dataset)
    indices = list(range(dataset_size))
    val_split = int(np.floor(val_size * dataset_size))
    if shuffle:
        np.random.seed(seed)
        np.random.shuffle(indices)
    train_indices = indices[val_split:]
    val_indices = indices[:val_split]

    # Creating PT data samplers and loaders:
    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)

    train_loader = DataLoader(dataset, batch_size=batch_train, sampler=train_sampler)
    val_loader = DataLoader(dataset, batch_size=batch_eval, sampler=val_sampler)
    return train_loader, val_loader


def read_spectrogram(spectrogram_file):
    # with open(spectrogram_file, "r") as f:
    spectrograms = np.load(spectrogram_file)
    # spectrograms contains a fused mel spectrogram and chromagram
    # Decompose as follows
    return spectrograms.T
def read_mel_spectrogram(spectrogram_file):
    # with open(spectrogram_file, "r") as f:
    spectrograms = np.load(spectrogram_file)
    mel_spectrogram=spectrograms[:128]
    # spectrograms contains a fused mel spectrogram and chromagram
    # Decompose as follows
    return mel_spectrogram.T
def read_chroma_spectrogram(spectrogram_file):
    # with open(spectrogram_file, "r") as f:
    spectrograms = np.load(spectrogram_file)
    chroma_spectrogram=spectrograms[128:]
    # spectrograms contains a fused mel spectrogram and chromagram
    # Decompose as follows
    return chroma_spectrogram.T


class LabelTransformer(LabelEncoder):
    def inverse(self, y):
        try:
            return super(LabelTransformer, self).inverse_transform(y)
        except:
            return super(LabelTransformer, self).inverse_transform([y])

    def transform(self, y):
        try:
            return super(LabelTransformer, self).transform(y)
        except:
            return super(LabelTransformer, self).transform([y])


class PaddingTransform(object):
    def __init__(self, max_length, padding_value=0):
        self.max_length = max_length
        self.padding_value = padding_value

    def __call__(self, s):
        if len(s) == self.max_length:
            return s

        if len(s) > self.max_length:
            return s[: self.max_length]

        if len(s) < self.max_length:
            s1 = copy.deepcopy(s)
            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)
            s1 = np.vstack((s1, pad))
            return s1


class SpectrogramDataset(Dataset):
    def __init__(
        self, path, class_mapping=None, train=True, max_length=-1, regression=None,read_value=read_spectrogram
    ):
        t = "train" if train else "test"
        p = os.path.join(path, t)
        self.regression = regression

        self.index = os.path.join(path, "{}_labels.txt".format(t))
        self.files, labels = self.get_files_labels(self.index, class_mapping)
        self.feats = [read_value(os.path.join(p, f)) for f in self.files]
        self.feat_dim = self.feats[0].shape[1]
        self.lengths = [len(i) for i in self.feats]
        self.max_length = max(self.lengths) if max_length <= 0 else max_length
        self.zero_pad_and_stack = PaddingTransform(self.max_length)
        self.label_transformer = LabelTransformer()
        if isinstance(labels, (list, tuple)):
            if not regression:
                self.labels = np.array(
                    self.label_transformer.fit_transform(labels)
                ).astype("int64")
            else:
                self.labels = np.array(labels).astype("float64")
            
    def get_files_labels(self, txt, class_mapping):
        with open(txt, "r") as fd:
            lines = [l.rstrip().split("\t") for l in fd.readlines()[1:]]
        files, labels = [], []
        for l in lines:
            if self.regression:
                l = l[0].split(",")
                files.append(l[0] + ".fused.full.npy")
                labels.append(l[self.regression])
                continue
            label = l[1]
            if class_mapping:
                label = class_mapping[l[1]]
            if not label:
                continue
            fname = l[0]
            if fname.endswith(".gz"):
                fname = ".".join(fname.split(".")[:-1])
            _id=l[0].split('.')[0]
            npy_file='{}.fused.full.npy'.format(_id)
            files.append(npy_file)
            labels.append(label)
        return files, labels

    def __getitem__(self, item):
        length = min(self.lengths[item], self.max_length)
        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], length

    def __len__(self):
        return len(self.labels)

"""bulding datasets"""
if __name__ == "__main__":
    
    # Dataset
    beat_mel_specs = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/', train=True,
                                         class_mapping=Class_Mapping, max_length=-1,
                                         read_value=read_mel_spectrogram)
    # Train and Test loaders
    train_loader_beat_mel, val_loader_beat_mel = torch_train_val_split(beat_mel_specs, 32, 32, val_size=.33)
    test_dataset_beat_mel = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/', train=False,
                                                 class_mapping=Class_Mapping, max_length=-1,
                                                 read_value=read_mel_spectrogram)
    test_loader_beat_mel = DataLoader(test_dataset_beat_mel, batch_size=1)
    
    ##################################################################################
    # load beat synced chroma chromagrams
    ##################################################################################
    beat_chroma = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/', train=True,
                                     class_mapping=Class_Mapping, max_length=-1,
                                     read_value=read_chroma_spectrogram)
    train_loader_beat_chroma, val_loader_beat_chroma = torch_train_val_split(beat_chroma, 32, 32, val_size=.33)
    test_dataset_beat_chroma = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/', train=False,
                                                 class_mapping=Class_Mapping, max_length=-1,
                                                 read_value=read_chroma_spectrogram)
    test_loader_beat_chroma = DataLoader(test_dataset_beat_chroma, batch_size=1)

    ##################################################################################
    # load fused speectrogram + chromagram for the full (non-beat-synced) data
    ##################################################################################
    specs_fused = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=True,
                                     class_mapping=Class_Mapping, max_length=-1,
                                     read_value=read_spectrogram)
    train_loader, val_loader = torch_train_val_split(specs_fused, 32, 32, val_size=.33)
    test_dataset = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=False,
                                     class_mapping=Class_Mapping, max_length=-1,
                                     read_value=read_spectrogram)
    test_loader = DataLoader(test_dataset, batch_size=1)
    
    ##################################################################################
    # load single synced mel spectrograms
    ##################################################################################
    # Dataset
    mel_specs = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=True,
                                        class_mapping=Class_Mapping, max_length=-1,
                                        read_value=read_mel_spectrogram)
    # Train and Test loaders
    train_loader_mel, val_loader_mel = torch_train_val_split(mel_specs, 32, 32, val_size=.33)
    test_dataset_mel = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=False,
                                            class_mapping=Class_Mapping, max_length=-1,
                                            read_value=read_mel_spectrogram)
    test_loader_mel = DataLoader(test_dataset_mel, batch_size=1)

import torch
import pickle
from torch.utils.data import Dataset
import torch.nn as nn
import warnings
warnings.filterwarnings("ignore")
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
from sklearn.metrics import classification_report

class CNN(nn.Module):

    def __init__(self):
    
        super(CNN, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 4, kernel_size=3),
            nn.BatchNorm2d(4),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
            
        )    
        self.layer2 = nn.Sequential(
            nn.Conv2d(4, 32, kernel_size=5),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)    
            )
        self.layer3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=5),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
            
        )
        self.layer4 = nn.Sequential(
            nn.Conv2d(64,128, kernel_size=3),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
            
        )
        
        self.fc1 = nn.Linear(49920, 10)
        
        
        
        
    def forward(self, x):
        # Forward function of CNN
        
        out = self.layer1(x)     # conv1
        out = self.layer2(out)   # conv2
        out = self.layer3(out)   # conv3
        out = self.layer4(out)   # conv4
        out = out.view(out.size(0), -1)
        out = self.fc1(out)      # fully-connected1
        return out
    
def training_CNN(trainloader,valloader,model_name,input_size,dataset,obover):
    if obover==True:
        maxseqlen = dataset.max_length
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #defining mnodel and the hyperparameters
        train_loss=[]
        num_classes=10
        num_epochs=200
        maxseqlen=trainloader.dataset.max_length

        model = CNN().to(device)
        model = model.double()


        # Loss and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)

        #take only 1 batch & transfer it to GPU
        feats, labels, lengths= next(iter(trainloader))
        feats=feats.reshape(-1,1,maxseqlen,input_size)
        feats, lengths, labels= feats.to(device),lengths.to(device),labels.to(device)

        for epoch in range(num_epochs):

            # Forward pass
            outputs = model.forward(feats)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss.append(round(loss.item(),4))

            #print results
            print('Epoch [{}/{}], Training Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))



        # Plotting learning curve
        running_loss=np.array(train_loss)
        plt.figure()
        plt.plot(running_loss, label="training loss")
        plt.xlabel('epoch')
        plt.ylabel('mean loss in the epoch')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)
        plt.show()




    elif obover==False:  
        model_name+= ".sav"

        #defining the device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        #defining the hyperparameters and the  model
        maxseqlen = -1
        num_classes = 10
        num_epochs = 30
        learning_rate = 0.001


        maxseqlen = dataset.max_length
        model = CNN().to(device)
        model = model.double()

        #loss_criterion and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        ts = len(trainloader)

        v_loss=[]
        t_loss=[]
        train_loss=[]

        for epoch in range(num_epochs):# loop over the dataset multiple times 
            with torch.no_grad():

                val_loss = 0.0
                best_val_loss = 1000000000
                total=0
                correct=0
                for i, (feats,labels,lengths) in enumerate(valloader):

                    feats_val = feats.reshape(-1,1, maxseqlen, input_size).to(device)
                    labels = labels.to(device) 
                    labels = torch.tensor(labels, dtype=torch.long, device=device)
                    lengths = lengths.to(device) 

                    # forward + validation_loss
                    outputs = model.forward(feats_val)
                    criterion = nn.CrossEntropyLoss()
                    loss = criterion(outputs, labels)
                    val_loss+= loss.item()

                    #checking the performance of the validation set at the end of each epoch
                    _, predicted = torch.max(outputs.data, -1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
                if epoch>=1:
                    print('Validation Accuracy after the end of the nu'+str(epoch)+' epoch= {} %'.format(100 * correct / total))

            v_loss.append(val_loss/len(valloader))

            if val_loss<=best_val_loss:
                best_val_loss = val_loss
                pickle.dump(model, open(model_name, 'wb'))


            running_loss=0.0
            train_loss.append([])
            for i,(feats, labels, lengths) in enumerate(trainloader):
                feats = feats.reshape(-1, 1,maxseqlen, input_size).to(device)
                labels = labels.to(device)
                labels = torch.tensor(labels, dtype=torch.long, device=device)
                lengths = lengths.to(device)

                #Forward_pass,Backward,Optimize
                outputs = model.forward(feats)
                loss = criterion(outputs, labels)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                if (i) % 8 == 0:
                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' 
                           .format(epoch+1, num_epochs, i+1, ts, loss.item()))
                train_loss[epoch].append(loss.item())

                # epoch loss
                running_loss += loss
                running_loss = running_loss / len(trainloader)

            t_loss.append(np.mean(train_loss))


        # Tranform losses to numpy arrays
        t_loss=np.array(t_loss)
        v_loss=np.array(v_loss)

        # Plotting learning curve
        plt.figure()
        plt.plot(t_loss, label="training loss")
        plt.plot(v_loss, label="validation loss")
        plt.xlabel('epoch')
        plt.ylabel('mean loss in the epoch')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)
        plt.show()
    
    return maxseqlen
            
            
            
            
            




def score_CNN(test_loader,input_size,model_name, maxseqlen):
    # Estimating the model with classification report
    model_name += ".sav"
    loaded_model = pickle.load(open(model_name, 'rb'))
    maxseqlen=test_loader.dataset.max_length
    
    # Disable batch normalization and dropout in testing
    loaded_model.eval()
    
    with torch.no_grad():
        correct = 0
        total = 0
        y_pred = []
        y_true = []
        for i,(feats, labels, lengths) in enumerate(test_loader):
            feats_test = feats.reshape(-1,1, maxseqlen, input_size).to(device)
            labels = labels.to(device)
            labels = torch.tensor(labels, dtype=torch.long, device=device)
            lengths = lengths.to(device)
            outputs2 = loaded_model.forward(feats_test)
            _, predicted = torch.max(outputs2.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            for pred in predicted:
                   y_pred.append(pred.item())
            for label in labels:
                  y_true.append(label.item())
    print(classification_report(y_true, y_pred))

maxseqlen = training_CNN(train_loader_mel, val_loader_mel, model_name="cnn_single_mel", input_size=128, dataset=mel_specs,obover=True)

maxseqlen = training_CNN(train_loader_mel, val_loader_mel, model_name="cnn_single_mel", input_size=128, dataset=mel_specs,obover=False)
score_CNN(test_loader=test_loader_mel, model_name="cnn_single_mel", input_size=128, maxseqlen=maxseqlen)



